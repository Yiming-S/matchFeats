---
title: "An Introduction to ‘matchFeats’"
author: "David Degras and Yiming Shen"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{An Introduction to matchFeats}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

This vignette presents a in-depth overview of the `matchFeats` package.

The `matchFeats` package's name is `One-To-One Feature Matching Under Association Ambiguity`. The package will provide four main functions which are Block Coordinate Ascent Method(`match.bca`), Gaussian Mixture Approach(`match.gaussmix`), K-Means-Like Method(`match.kmeans`) and Template-Based Method(`match.template`).

The kinds of statistical methods to match feature vectors between sample units under association ambiguity. Applications include object tracking, video surveillance, remote sensing as well as multilevel modeling. Given a fixed number of classes/distributions, for each unit, exactly one vector of each class is observed without label. The goal is to label the feature vectors using each label exactly once so to produce the best match across units, e.g. by minimizing the variability within classes. Several statistical solutions based on empirical loss functions and probabilistic modeling are provided.


# How to install the package

If you would like to install the development version of `matchFeats`, you may do so
by using, for example, `devtools`:

``` {r eval = F} 
# install.packages("devtools")
install.packages("clue") # run this line if you don’t have package 'clue' installed yet
install.packages("foreach") # run this line if you don’t have package 'foreach' installed yet
install.packages("devtools") # run this line if you don’t have package 'devtools' installed yet
library(devtools)
devtools::install_github("ddegras/matchFeats")
```

After successful installation, we’ll start by loading `matchFeats`:
```{r}
library(clue)
library(matchFeats)
```

# How to use the package

Below we will give an overview of all those statistical methods and, further in the document, we will present some usage examples.

## Functions 

### Processing data description 

The functions will process **n** matrices which like the **c** columns and **r** rows:

```{r}
## Generate data
n <- 5 # 5 matrices
c <- 3  # Each matrix has 3 columns
r <- 5  # Each matrix has 5 rows
mu <- matrix(1:c, nrow=r, ncol=c, byrow=TRUE) 
sigma <- 0.3
x <- array(mu, c(r,c,n)) + rnorm(r*c*n,sigma)
x
```

### Block Coordinate Ascent Method(`match.bca`)

#### Details

Given a set of \eqn{n} statistical units, each having \eqn{m} possibly mislabeled feature vectors, the one-to-one matching problem is to find a set of \eqn{n} label permutations that produce the best match of feature vectors across units. The objective function to minimize is the sum of (weighted) squared Euclidean distances between all pairs of feature vectors having the same (new) label. This amounts to minimizing the sum of the within-label variances.  
The sample means and sample covariances of the matched feature vectors are calculated as a post-processing step.  
	
The block-coordinate ascent (BCA) algorithm successively sweeps through the statistical units (=blocks), each time relabeling the \eqn{m} feature vectors of a unit to best match those of the other \eqn{n-1} units. 

If \code{x} is a matrix, the rows should be sorted by increasing unit label and  \code{unit} should be a nondecreasing sequence of integers, for example \eqn{(1,...,1,2,...,2,...,n,...,n)} with each integer \eqn{1,...,n} replicated \eqn{m} times. 

The argument \code{w} can be specified as a vector of positive numbers (will be recycled to length \eqn{p} if needed) or as a positive definite matrix of size \eqn{(p,p)}.

To calculate a common sample covariance for all matched feature vectors, set \code{equal.variance} to TRUE; otherwise (FALSE) one sample covariance matrix is calculated for each label/class. 

The optional argument \code{control} is a list with two fields: \code{sigma}, starting point for the optimization (\eqn{(m,n)} matrix of permutations; and \code{maxit}, maximum number of iterations/sweeps. 

#### Equation 
Let $\Pi$ be the set of all $k \times k$ permutation matrices.
$$\displaystyle \min_{P_1,...,P_n \in \Pi} \sum_{i=1}^n \sum_{j=1}^n ||X_iP_i-X_jP_j||_F^2$$

Where $||X||_F = (tr(X'X))^{\frac{1}{2}}$


$$
\begin{eqnarray*}
\displaystyle \sum_{i=1}^n \sum_{j=1}^n ||X_iP_i-X_jP_j||_F^2 &=& \sum_{i=1}^n \sum_{j=1}^n (||X_iP_i||^2_F + ||X_jP_j||^2_F - 2tr(P'_i X'_i X_jP_j)) \\ 
&=& \sum_{i=1}^n \sum_{j=1}^n (||X_i||^2_F + ||X_j||^2_F - 2tr(P'_i X'_i X_jP_j)) \\ 
&=& \sum_{i=1}^n \sum_{j=1}^n (||X_i||^2_F + ||X_j||^2_F ) - 2|| \sum_{i=1}^n X_iP_i||_F^2
\end{eqnarray*}
$$

$$
\begin{eqnarray*}
\displaystyle \max_{P_1,...,P_n \in \Pi}||\sum_{i=1}^nX_iP_i||^2_F
\end{eqnarray*}
$$

$$
\begin{eqnarray*}
\displaystyle \max_{P_i \in \Pi}||X_iP_i+\sum_{1\leq j \leq n \ j \neq i} X_j \hat{P_j}||_F^2
\end{eqnarray*}
$$

$$
\begin{eqnarray*}
\displaystyle \max_{P_i \in \Pi}trP_i'(X_i'\sum_{j \neq i} X_j \hat{P_j})
\end{eqnarray*}
$$

#### Example
```{r}
# Match all feature vectors with Block Coordinate Ascent Method
bcaResult <- match.bca(x)

# Display results
bcaResult

# re-arranged (matched) feature vectors
bcamatched <- array(dim=dim(x)) 
for (i in 1:n)
	bcamatched[,,i] <- x[,bcaResult$sigma[,i],i]

# Display matched results
bcamatched

# Display the summary result
summary(bcaResult)
```

### Gaussian Mixture Approach(`match.gaussmix`)

#### Details

Given a sample of n statistical units, each having m possibly mislabeled feature vectors, the one-to-one matching problem is to find a set of n label permutations that produce the best match of feature vectors across units. This problem is sometimes referred to as "data association ambiguity".

The feature vectors of all units are represented as independent realizations of m multivariate normal distributions with unknown parameters. For each sample unit, exactly one vector from each distribution is observed and the m corresponding labels are randomly permuted. The goal is to estimate the true class of each feature vector, as well as the mean vector and covariance matrix of each distribution. These quantities are evaluated by ML estimation via the Expectation-Maximization (EM) algorithm.

If x is a matrix, the rows should be sorted by increasing unit label and unit should be a nondecreasing sequence of integers, for example (1,...,1,2,...,2,...,n,...,n) with each integer 1,...,n replicated m times.

The arguments mu and V should be specified only if a good guess is available for these parameters. Otherwise bad starting values may cause the EM algorithm to converge to a local maximum of the likelihood function quite far from the global maximum.

If method is set to exact (default), the class probabilities of the feature vectors (given the data) are calculated exactly at each iteration of the EM algorithm. This operation can be slow as it involves calculating the permanent of matrices. The argument method can be set to approximate to speed up calculations, but this option is not recommended in general as the approximations used are very crude and may produce "bad" EM solutions.

The optional argument control can be specified with these fields: maxit, maximum number of EM iterations (default=1e4); eps, relative tolerance for EM convergence (default=1e-8), the EM algorithm stops if the relative increase in log-likelihood between two iterations is less than this tolerance; verbose, set to TRUE to display algorithm progress (default=FALSE).

#### Equation 
$$logL_c = \sum_{i=1}^n \sum_{k=1}^m \sum_{l=1}^m log\varphi(x_{ik};\mu_l,\Sigma_l)I_{ikl}$$
EM algorithm

$\hat{\theta} ={(\hat{\mu},\hat{\Sigma_l} : l \in[m])}$

E step.

$$
\begin{eqnarray*}
E_{\hat{\theta}}(I_{ikl}|X_i) &=& P_{\hat{\theta}}(I_{ikl}=1|X_i)\\
&=& \frac{P_{\hat{\theta}}(X_i|I_{ikl}=1)P_{\hat{\theta}}(I_{ikl}=1)}{P_{\hat{\theta}}(X_i)} \\
&=& c_iP_{\hat{\theta}}(X_i|I_{ikl}=1) \\
&=& c_i \sum_{\sigma \in S: \sigma(k)=l}P_{\hat \theta}(X_i|I_{il\sigma(1)}=1,...,I_{im\sigma(m)} =1) \\
&\times& P_{\hat \theta}(I_{il\sigma(1)}=1,...,I_{im\sigma(m)} =1|I_{ikl}=1) \\
&=& \frac{c_i}{(m-1)!}\sum_{\sigma \in S: \sigma(k)=l}\prod_{r=1}^m P_{\hat {\theta}} (x_{ir}|I_{ir \sigma(r)}=1) \\
&=& \frac{c_i}{(m-1)!}\sum_{\sigma \in S: \sigma(k)=l}\prod_{r=1}^m \varphi (x_{ir};\hat{\mu}_{\sigma(r)},\hat{\Sigma}_{\sigma(r)})
\end{eqnarray*}
$$

$$
\begin{eqnarray*}
E_{\hat{\theta}}(I_{ikl}|X_i) &\approx& E_{\hat{\theta}}(I_{ikl}|x_{ik})\\
&=& \frac{P_{\hat{\theta}}(x_{ik}|I_{ikl}=1)P_{\hat{\theta}}(I_{ikl}=1)}{P(x_{ik})} \\
&=& c_{ik}P_{\hat{\theta}}(x_{ik}|I_{ikl}=1) \\
&=& c_{ik} \varphi(x_{ik};\hat \mu_l,\hat \Sigma_l)
\end{eqnarray*}
$$
$$
\begin{eqnarray*}
E_{\hat{\theta}}(I_{ikl}|X_i) &\approx& \frac{c_i}{(m-1)!} \max_{\sigma \in S:\sigma(k)=l}\prod^m_{r=1}\varphi(x_{ir}; \hat \mu_{\sigma(r)},\hat \Sigma_{\sigma(r)}) \\
\end{eqnarray*}
$$
M step
$$
\begin{eqnarray*}
&\mu^+_l& =  \frac{1}{n} \sum^n_{i=1} \sum^m_{k=1}P_{\hat \theta}(I_{ikl}=1|X_i)x_{ik} \\
&\Sigma^+_l& =  \frac{1}{n} \sum^n_{i=1} \sum^m_{k=1}P_{\hat \theta}(I_{ikl}=1|X_i)(x_{ik}-\mu^+_l)(x_{ik}-\mu^+_l)'
\end{eqnarray*}
$$
Log-likelihood
$$
\begin{eqnarray*}
\log L(\hat \theta) = \sum^n_{i=1}\log \left(\frac{1}{m!} \sum_{\sigma \in S} \prod^m_{k=1} \varphi(x_{ik};\hat \mu_{\sigma(k)},\hat \Sigma_{\sigma(k)}) \right)
\end{eqnarray*}
$$

#### Example
```{r}
# match.gaussmix(x,100)
```


### K-Means-Like Method(`match.kmeans`)

#### Details

Given a sample of \eqn{n} statistical units, each having \eqn{m} possibly mislabeled feature vectors, the one-to-one matching problem is to find a set of \eqn{n} label permutations that produce the best match of feature vectors across units. This problem is sometimes referred to as "data association ambiguity". 

The feature vectors of all units are represented as independent realizations of \eqn{m} multivariate normal distributions with unknown parameters. For each sample unit, exactly one vector from each distribution is observed and the \eqn{m} corresponding labels are randomly permuted. The goal is to estimate the true class of each feature vector, as well as the mean vector and covariance matrix of each distribution. These quantities are evaluated by ML estimation via the Expectation-Maximization (EM) algorithm. 

If \code{x} is a matrix, the rows should be sorted by increasing unit label and  \code{unit} should be a nondecreasing sequence of integers, for example \eqn{(1,...,1,2,...,2,...,n,...,n)} with each integer \eqn{1,...,n} replicated \eqn{m} times. 

The arguments \code{mu} and \code{V} should be specified only if a good guess is available for these parameters. Otherwise bad starting values may cause the EM algorithm to converge to a local maximum of the likelihood function quite far from the global maximum. 

If \code{method} is set to \code{exact} (default), the class probabilities of the feature vectors (given the data) are calculated exactly at each iteration of the EM algorithm. This operation can be slow as it involves calculating the permanent of matrices. The argument \code{method} can be set to \code{approximate} to speed up calculations, but this option is not recommended in general as the approximations used are very crude and may produce "bad" EM solutions. 

The optional argument \code{control} can be specified with these fields: 
\code{maxit}, maximum number of EM iterations (default=1e4); 
\code{eps}, relative tolerance for EM convergence (default=1e-8), 
the EM algorithm stops if the relative increase in log-likelihood between two iterations is less than this tolerance; \code{verbose}, set to TRUE to display
 algorithm progress (default=FALSE). 
 
#### Equation 

$\displaystyle \min_{\sigma_1,...,\sigma_n} \sum_{i=1}^n \sum_{l=1}^k ||X_{i\sigma_i(l)}-\overline{x}_{\sigma,l}||^2$


#### Example

```{r}
# Match all feature vectors with K-Means-Like Method
kResult <- match.kmeans(x)

# Display results
kResult

# re-arranged (matched) feature vectors
kmatched <- array(dim=dim(x)) 
for (i in 1:n)
	kmatched[,,i] <- x[,kResult$sigma[,i],i]

# Display matched results
kmatched

# Display the summary result
summary(match.kmeans(x))
```

### Template-Based Method(`match.template`)

#### Details

Given a sample of \eqn{n} statistical units, each having \eqn{m} possibly mislabeled feature vectors, the one-to-one matching problem is to find a set of \eqn{n} label permutations that produce the best match of feature vectors across units. The objective function to minimize is the sum of squared (Euclidean) distances between all feature vectors having the same (new) label. This amounts to minimizing the sum of the within-label variances.  
	
The template-based method consists in relabeling successively each sample unit to best match a template matrix of feature vectors. This method is very fast but its optimization performance is only as good as the template. A template that is typical or representative of the collected data. 

If \code{x} is a matrix, the rows should be sorted by increasing unit label and  \code{unit} should be a nondecreasing sequence of integers, for example \eqn{(1,...,1,2,...,2,...,n,...,n)} with each integer \eqn{1,...,n} replicated \eqn{m} times. 

The argument \code{w} can be specified as a vector of positive numbers (will be recycled to length \eqn{p} if needed) or as a positive definite matrix of size \eqn{(p,p)}.

#### Example

Import matrices, then:
```{r}
# Match all feature vectors with first case as template
TemplateResult <- match.template(x,1)

# Display results
TemplateResult

Tmatched <- array(dim=dim(x)) 
# re-arranged (matched) feature vectors
for (i in 1:n)
	Tmatched[,,i] <- x[,TemplateResult$sigma[,i],i]

# Display matched results
Tmatched

# Display the summary result
summary(match.template(x,1))
```

### One-Pass Algorithm(`match.1pass`)

#### Details

#### Equation

#### Example
```{r}
## Use one pass function with data in matrix form
Oneresult <- match.1pass(x, unit=n)

# Display results
Oneresult

# re-arranged (matched) feature vectors
Onematched <- array(dim=dim(x)) 
for (i in 1:n)
	Onematched[,,i] <- x[,Oneresult$sigma[,i],i]

# Display matched results
Onematched

# Display the summary result
summary(Oneresult)
```

# Handwriting example project

The data comes from UCI - Optical Recognition of Handwritten Digits Data Set which used to test matchFeats packages. 
There are 3823 training data and 1797 testing data in this handwritten digits data set example projec.  Each data set has 
```{r}
train_url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra"
train <- read.table(train_url,sep = ",")
str(train)
```


# References

* [Degras and Shen (2020). One-to-one feature matching under association ambiguity.](web)
